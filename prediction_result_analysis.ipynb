{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction Result Analysis (CS2 Reviews)\n",
        "\n",
        "This notebook provides deep insights into the performance of the sentiment analysis model on Counter-Strike 2 reviews.\n",
        "We move beyond basic EDA and focus on understanding *how* and *why* the model makes specific predictions.\n",
        "\n",
        "**Theme:** Asiimov (Orange, Black, White, Grey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# --- Asiimov Color Scheme ---\n",
        "asiimov_colors = {\n",
        "    'orange': '#FF9900',\n",
        "    'black': '#1A1A1A',\n",
        "    'white': '#FFFFFF',\n",
        "    'grey': '#5c5c5c',\n",
        "    'light_grey': '#d1d1d1'\n",
        "}\n",
        "\n",
        "# Setup Plotly Template\n",
        "pio.templates[\"asiimov\"] = go.layout.Template(\n",
        "    layout=go.Layout(\n",
        "        colorway=[asiimov_colors['orange'], asiimov_colors['black'], asiimov_colors['grey']],\n",
        "        plot_bgcolor=asiimov_colors['white'],\n",
        "        paper_bgcolor=asiimov_colors['white'],\n",
        "        font={'color': asiimov_colors['black']},\n",
        "        title={'font': {'color': asiimov_colors['black']}},\n",
        "        xaxis={'gridcolor': asiimov_colors['light_grey'], 'linecolor': asiimov_colors['black']},\n",
        "        yaxis={'gridcolor': asiimov_colors['light_grey'], 'linecolor': asiimov_colors['black']},\n",
        "    )\n",
        ")\n",
        "pio.templates.default = \"asiimov\"\n",
        "\n",
        "print(\"Environment Setup Complete. Asiimov Theme Applied.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Predictions\n",
        "df = pd.read_csv('cs2_10k_predictions.csv')\n",
        "\n",
        "# --- Preprocessing ---\n",
        "# Ensure ground truth is integer (1 for True/Positive, 0 for False/Negative)\n",
        "df['actual_label'] = df['voted_up'].astype(int)\n",
        "\n",
        "# Determine correctness\n",
        "df['is_correct'] = df['actual_label'] == df['predicted_label']\n",
        "df['result_type'] = df.apply(lambda x: 'TP' if x['actual_label']==1 and x['predicted_label']==1 else\n",
        "                                     'TN' if x['actual_label']==0 and x['predicted_label']==0 else\n",
        "                                     'FP' if x['actual_label']==0 and x['predicted_label']==1 else\n",
        "                                     'FN', axis=1)\n",
        "\n",
        "# Calculate review length\n",
        "# Using 'cleaned_review' for consistency\n",
        "df['review_length'] = df['cleaned_review'].fillna('').astype(str).apply(len)\n",
        "\n",
        "print(f\"Loaded {len(df)} predictions.\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insight 1: Performance Overview (Confusion Matrix)\n",
        "We visualize the Confusion Matrix to see the balance of True Positives, True Negatives, False Positives, and False Negatives.\n",
        "We calculate metrics like Precision and Recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(df['actual_label'], df['predicted_label'])\n",
        "labels = ['Negative', 'Positive']\n",
        "\n",
        "# Create annotated heatmap\n",
        "fig = px.imshow(cm, text_auto=True, \n",
        "                labels=dict(x=\"Predicted Label\", y=\"Actual Label\", color=\"Count\"),\n",
        "                x=labels, y=labels,\n",
        "                color_continuous_scale=[[0, asiimov_colors['white']], [1, asiimov_colors['orange']]]\n",
        "               )\n",
        "fig.update_layout(title_text=\"Confusion Matrix Heatmap\")\n",
        "fig.show()\n",
        "\n",
        "# Print text report\n",
        "print(classification_report(df['actual_label'], df['predicted_label'], target_names=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insight 2: Prediction Confidence Analysis\n",
        "How confident is the model? We plot the distribution of predicted probabilities.\n",
        "Ideally, we want the model to be confident (near 0 or 1). Predictions near 0.5 indicate uncertainty.\n",
        "We separate the distributions by the **Actual Label** to visualize separation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histogram of probabilities for Positive vs Negative Ground Truth\n",
        "fig = px.histogram(df, x=\"predicted_prob\", color=\"voted_up\", \n",
        "                   nbins=50, \n",
        "                   marginal=\"box\", # Box plot on top\n",
        "                   opacity=0.7,\n",
        "                   color_discrete_map={True: asiimov_colors['orange'], False: asiimov_colors['black']},\n",
        "                   labels={'voted_up': 'Actual Sentiment'},\n",
        "                   title=\"Prediction Probability Distribution by Actual Sentiment\")\n",
        "fig.update_layout(barmode='overlay')\n",
        "fig.update_traces(marker_line_width=0)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insight 3: Accuracy vs. Review Length\n",
        "Does the model perform better on longer or shorter reviews?\n",
        "We bin reviews by length and calculate the accuracy for each bin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bins for review length\n",
        "bins = [0, 50, 100, 200, 500, 1000, 5000]\n",
        "labels_len = ['0-50', '50-100', '100-200', '200-500', '500-1000', '1000+']\n",
        "df['length_bin'] = pd.cut(df['review_length'], bins=bins, labels=labels_len)\n",
        "\n",
        "# Calculate accuracy per bin\n",
        "acc_by_len = df.groupby('length_bin')['is_correct'].mean().reset_index()\n",
        "\n",
        "fig = px.line(acc_by_len, x='length_bin', y='is_correct', markers=True,\n",
        "              title=\"Model Accuracy by Review Length\",\n",
        "              labels={'length_bin': 'Review Length (Characters)', 'is_correct': 'Accuracy'},\n",
        "              color_discrete_sequence=[asiimov_colors['orange']])\n",
        "fig.update_yaxes(tickformat=\".1%\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insight 4: Error Analysis (Confident Mistakes)\n",
        "We examine cases where the model was **very confident** (prob > 0.90 or < 0.10) but **incorrect**.\n",
        "These \"confident errors\" often reveal specific weaknesses (e.g., sarcasm, misleading keywords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for confident errors\n",
        "confident_fp = df[(df['result_type'] == 'FP') & (df['predicted_prob'] > 0.90)]\n",
        "confident_fn = df[(df['result_type'] == 'FN') & (df['predicted_prob'] < 0.10)]\n",
        "\n",
        "print(f\"Confident False Positives (Predicted Positive, Actual Negative): {len(confident_fp)}\")\n",
        "print(f\"Confident False Negatives (Predicted Negative, Actual Positive): {len(confident_fn)}\")\n",
        "\n",
        "print(\"\\n--- Examples of Confident False Positives (Model thought it was GOOD, but it was BAD) ---\")\n",
        "for i, row in confident_fp.head(3).iterrows():\n",
        "    print(f\"[Prob: {row['predicted_prob']:.4f}] Review: {row['cleaned_review'][:200]}...\")\n",
        "\n",
        "print(\"\\n--- Examples of Confident False Negatives (Model thought it was BAD, but it was GOOD) ---\")\n",
        "for i, row in confident_fn.head(3).iterrows():\n",
        "    print(f\"[Prob: {row['predicted_prob']:.4f}] Review: {row['cleaned_review'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insight 5: Calibration Curve\n",
        "A reliability diagram to check if the predicted probabilities are well-calibrated.\n",
        "If the model predicts 0.7 for a set of samples, ~70% of them should actually be positive.\n",
        "- **Perfectly Calibrated:** Diagonal dotted line.\n",
        "- **S-shape:** Model is under-confident or over-confident."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prob_true, prob_pred = calibration_curve(df['actual_label'], df['predicted_prob'], n_bins=10)\n",
        "\n",
        "calibration_df = pd.DataFrame({'Mean Predicted Probability': prob_pred, 'Fraction of Positives': prob_true})\n",
        "\n",
        "fig = px.line(calibration_df, x='Mean Predicted Probability', y='Fraction of Positives',\n",
        "              markers=True, title=\"Calibration Curve (Reliability Diagram)\",\n",
        "              color_discrete_sequence=[asiimov_colors['orange']])\n",
        "\n",
        "# Add diagonal reference line\n",
        "fig.add_shape(type=\"line\", x0=0, y0=0, x1=1, y1=1,\n",
        "              line=dict(color=asiimov_colors['grey'], width=2, dash=\"dash\"))\n",
        "\n",
        "fig.update_layout(xaxis_range=[0, 1], yaxis_range=[0, 1])\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}