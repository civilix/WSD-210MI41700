{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Sentiment Analysis Insights\n",
    "\n",
    "This notebook explores deeper insights into the sentiment model's performance, focusing on error drivers, temporal trends, and probability calibration.\n",
    "\n",
    "**Theme:** Asiimov (Orange, Black, White)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Define Asiimov Color Palette\n",
    "asiimov_colors = {\n",
    "    'orange': '#FF9900',\n",
    "    'black': '#1a1a1a',\n",
    "    'white': '#ffffff',\n",
    "    'grey': '#5c5c5c',\n",
    "    'light_grey': '#d1d1d1'\n",
    "}\n",
    "\n",
    "# Set default template\n",
    "pio.templates[\"asiimov\"] = go.layout.Template(\n",
    "    layout=go.Layout(\n",
    "        colorway=[asiimov_colors['orange'], asiimov_colors['black'], asiimov_colors['grey']],\n",
    "        plot_bgcolor=asiimov_colors['white'],\n",
    "        paper_bgcolor=asiimov_colors['white'],\n",
    "        font={'color': asiimov_colors['black']},\n",
    "        title={'font': {'color': asiimov_colors['black']}},\n",
    "        xaxis={'gridcolor': '#e0e0e0', 'showgrid': True},\n",
    "        yaxis={'gridcolor': '#e0e0e0', 'showgrid': True}\n",
    "    )\n",
    ")\n",
    "pio.templates.default = \"asiimov\"\n",
    "\n",
    "print(\"Libraries loaded and Asiimov theme defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "df = pd.read_csv('cs2_10k_predictions.csv')\n",
    "\n",
    "df['actual_label'] = df['voted_up'].astype(int)\n",
    "df['is_correct'] = df['actual_label'] == df['predicted_label']\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_created'], unit='s')\n",
    "\n",
    "print(\"Data loaded. Rows:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 1: Keyword Error Analysis\n",
    "What specific words are associated with the model's mistakes? We analyze the most frequent terms in False Positives and False Negatives to understand what confuses the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Stopwords list\n",
    "STOPWORDS = set([\n",
    "    'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'of', 'in', 'on', 'at', 'to', 'for', \n",
    "    'with', 'by', 'it', 'this', 'that', 'i', 'you', 'he', 'she', 'they', 'we', 'my', 'your', 'his', 'her', \n",
    "    'their', 'our', 'game', 'play', 'cs2', 'cs', 'counter', 'strike', 'valve', 'steam', 'be', 'have', 'has',\n",
    "    'not', 'no', 'so', 'just', 'like', 'good', 'bad', 'very', 'much', 'get', 'got', 'do', 'does', 'did',\n",
    "    'can', 'will', 'would', 'if', 'when', 'from', 'out', 'up', 'down', 'about', 'than', 'then', 'now', 'go',\n",
    "    'global', 'offensive', 'fps', 'playing', 'played', 'time', 'really', 'even', 'still', 'one', 'all', 'me', 'im', 'its',\n",
    "    'review', '10', '0', 'best', 'worst', 'trash', 'shit', 'fun', 'great', 'nice', 'love', 'recommend', 'dont', 'cant'\n",
    "])\n",
    "\n",
    "def get_top_words(texts, n=10):\n",
    "    all_text = \" \".join(texts).lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', all_text)\n",
    "    words = [w for w in words if w not in STOPWORDS and len(w) > 2]\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# False Positives: Actual Negative (0), Predicted Positive (1)\n",
    "fp_reviews = df[(df['actual_label'] == 0) & (df['predicted_label'] == 1)]['clean_review'].astype(str).tolist()\n",
    "fp_data = pd.DataFrame(get_top_words(fp_reviews), columns=['Word', 'Count'])\n",
    "fp_data['Error Type'] = 'False Positive (Predicted Pos, Actual Neg)'\n",
    "\n",
    "# False Negatives: Actual Positive (1), Predicted Negative (0)\n",
    "fn_reviews = df[(df['actual_label'] == 1) & (df['predicted_label'] == 0)]['clean_review'].astype(str).tolist()\n",
    "fn_data = pd.DataFrame(get_top_words(fn_reviews), columns=['Word', 'Count'])\n",
    "fn_data['Error Type'] = 'False Negative (Predicted Neg, Actual Pos)'\n",
    "\n",
    "error_keywords = pd.concat([fp_data, fn_data])\n",
    "\n",
    "fig = px.bar(\n",
    "    error_keywords,\n",
    "    x='Count',\n",
    "    y='Word',\n",
    "    color='Error Type',\n",
    "    orientation='h',\n",
    "    title='Top Keywords in Prediction Errors',\n",
    "    color_discrete_map={\n",
    "        'False Positive (Predicted Pos, Actual Neg)': asiimov_colors['orange'],\n",
    "        'False Negative (Predicted Neg, Actual Pos)': asiimov_colors['black']\n",
    "    },\n",
    "    barmode='group'\n",
    ")\n",
    "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "Words like \"cheaters\", \"hackers\", and \"fix\" appear frequently in errors. This suggests the model struggles when players complain about specific issues (like cheaters) but might still vote 'Recommended' (or vice versa, praising the game but voting 'Not Recommended' due to a specific grievance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 2: Temporal Sentiment Trend\n",
    "How has the model's perceived sentiment changed over time? We look at the rolling average of positive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample by month to see trends\n",
    "monthly_sentiment = df.set_index('timestamp').resample('ME')['predicted_label'].mean().reset_index()\n",
    "monthly_sentiment.columns = ['Date', 'Positive Sentiment Rate']\n",
    "\n",
    "fig = px.line(\n",
    "    monthly_sentiment,\n",
    "    x='Date',\n",
    "    y='Positive Sentiment Rate',\n",
    "    title='Trend of Positive Sentiment Over Time (Model Prediction)',\n",
    "    color_discrete_sequence=[asiimov_colors['orange']]\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "The chart shows how the sentiment evolves. Significant dips might correlate with controversial updates or ban waves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 3: Probability Calibration\n",
    "Is the model's confidence reliable? When it predicts a 90% probability of being positive, is it actually positive 90% of the time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin probabilities into 10 bins\n",
    "df['prob_bin'] = pd.cut(df['predicted_prob'], bins=10)\n",
    "\n",
    "# Calculate actual positive rate for each bin\n",
    "calibration = df.groupby('prob_bin', observed=False).agg(\n",
    "    avg_pred_prob=('predicted_prob', 'mean'),\n",
    "    actual_pos_rate=('actual_label', 'mean'),\n",
    "    count=('actual_label', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Filter out empty bins\n",
    "calibration = calibration[calibration['count'] > 0]\n",
    "\n",
    "fig = px.line(\n",
    "    calibration,\n",
    "    x='avg_pred_prob',\n",
    "    y='actual_pos_rate',\n",
    "    markers=True,\n",
    "    title='Model Probability Calibration (Reliability Diagram)',\n",
    "    labels={'avg_pred_prob': 'Mean Predicted Probability', 'actual_pos_rate': 'Actual Positive Fraction'},\n",
    "    color_discrete_sequence=[asiimov_colors['black']]\n",
    ")\n",
    "\n",
    "# Add a perfect calibration line\n",
    "fig.add_shape(\n",
    "    type=\"line\", line=dict(dash=\"dash\", color=asiimov_colors['grey']),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "A perfectly calibrated model would follow the diagonal dashed line. Deviations indicate over-confidence or under-confidence. If the curve is below the diagonal, the model is over-estimating the probability of the positive class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}