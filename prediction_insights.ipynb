{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Prediction Insights\n",
    "\n",
    "This notebook analyzes the predictions of the sentiment analysis model on Counter-Strike 2 reviews. \n",
    "The goal is to derive insights into model performance, error patterns, and temporal trends.\n",
    "\n",
    "**Color Scheme (Asiimov):**\n",
    "- Orange: #FF9900\n",
    "- Black: #1A1A1A\n",
    "- White: #FFFFFF\n",
    "- Grey: #5c5c5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Asiimov Color Scheme\n",
    "ASIIMOV_ORANGE = '#FF9900'\n",
    "ASIIMOV_BLACK = '#1A1A1A'\n",
    "ASIIMOV_WHITE = '#FFFFFF'\n",
    "ASIIMOV_GREY = '#5c5c5c'\n",
    "\n",
    "# Set default plotly template or update layout in functions\n",
    "def apply_asiimov_theme(fig):\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor=ASIIMOV_BLACK,\n",
    "        paper_bgcolor=ASIIMOV_BLACK,\n",
    "        font=dict(color=ASIIMOV_WHITE),\n",
    "        title_font=dict(color=ASIIMOV_ORANGE),\n",
    "        xaxis=dict(gridcolor=ASIIMOV_GREY, showgrid=True),\n",
    "        yaxis=dict(gridcolor=ASIIMOV_GREY, showgrid=True),\n",
    "        legend=dict(bgcolor=ASIIMOV_BLACK, bordercolor=ASIIMOV_GREY)\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data\n",
    "We load the prediction results and convert timestamps to datetime objects for time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CS2Review_clean_predictions_100k.csv')\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['date'] = pd.to_datetime(df['timestamp_created'], unit='s')\n",
    "\n",
    "# Ensure voted_up is int for calculation\n",
    "df['voted_up_int'] = df['voted_up'].astype(int)\n",
    "\n",
    "# Calculate correctness\n",
    "df['is_correct'] = (df['voted_up_int'] == df['predicted_label'])\n",
    "\n",
    "print(f\"Data loaded: {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Analysis: Sentiment Decoupling\n",
    "We analyze the divergence between the ground truth user sentiment (Vote) and the model's predicted sentiment based on text (Label). \n",
    "A divergence indicates periods where user ratings might not align with the textual content (e.g., review bombing, irony) or where the model performance degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample by week to smooth out noise\n",
    "df_resampled = df.set_index('date').resample('W').agg({\n",
    "    'voted_up_int': 'mean',\n",
    "    'predicted_label': 'mean',\n",
    "    'recommendationid': 'count'\n",
    "}).rename(columns={'recommendationid': 'count'})\n",
    "\n",
    "# Filter out weeks with very few reviews\n",
    "df_resampled = df_resampled[df_resampled['count'] > 50]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_resampled.index,\n",
    "    y=df_resampled['voted_up_int'],\n",
    "    mode='lines',\n",
    "    name='Ground Truth (User Vote)',\n",
    "    line=dict(color=ASIIMOV_ORANGE, width=3)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_resampled.index,\n",
    "    y=df_resampled['predicted_label'],\n",
    "    mode='lines',\n",
    "    name='Model Prediction',\n",
    "    line=dict(color=ASIIMOV_WHITE, width=2, dash='dot')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Decoupling Analysis: Ground Truth vs Model Prediction (Weekly)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Positive Rate',\n",
    "    yaxis_tickformat='.0%'\n",
    ")\n",
    "\n",
    "apply_asiimov_theme(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Disagreement Rate Over Time\n",
    "This visualization highlights when the model disagrees most with the user votes. High disagreement peaks may correspond to game updates or events where user sentiment is complex (e.g., mixed feelings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disagreement'] = (df['voted_up_int'] != df['predicted_label']).astype(int)\n",
    "df_disagreement = df.set_index('date').resample('W')['disagreement'].mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_disagreement.index,\n",
    "    y=df_disagreement,\n",
    "    mode='lines',\n",
    "    name='Disagreement Rate',\n",
    "    fill='tozeroy',\n",
    "    line=dict(color=ASIIMOV_ORANGE, width=2),\n",
    "    fillcolor='rgba(255, 153, 0, 0.2)'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Disagreement Rate Over Time',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Disagreement Rate',\n",
    "    yaxis_tickformat='.0%'\n",
    ")\n",
    "\n",
    "apply_asiimov_theme(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confidence Analysis: Correct vs Incorrect Predictions\n",
    "Are errors caused by low confidence (uncertainty) or high confidence (wrong interpretation)? We compare the distribution of the model's `positive_probability` for correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_probs = df[df['is_correct']]['positive_probability']\n",
    "incorrect_probs = df[~df['is_correct']]['positive_probability']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=correct_probs,\n",
    "    name='Correct Predictions',\n",
    "    marker_color=ASIIMOV_GREY,\n",
    "    opacity=0.7,\n",
    "    xbins=dict(start=0, end=1, size=0.05)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=incorrect_probs,\n",
    "    name='Incorrect Predictions',\n",
    "    marker_color=ASIIMOV_ORANGE,\n",
    "    opacity=0.7,\n",
    "    xbins=dict(start=0, end=1, size=0.05)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Probability Distribution: Correct vs Incorrect',\n",
    "    xaxis_title='Predicted Positive Probability',\n",
    "    yaxis_title='Count',\n",
    "    barmode='overlay'\n",
    ")\n",
    "\n",
    "apply_asiimov_theme(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Review Length vs Prediction Accuracy\n",
    "Does the model perform better on longer, more detailed reviews compared to short ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_length'] = df['clean_review'].astype(str).str.len()\n",
    "\n",
    "# Bin review lengths (0-50, 50-100, 100-200, 200-500, 500+)\n",
    "bins = [0, 50, 100, 200, 500, 10000]\n",
    "labels = ['0-50', '50-100', '100-200', '200-500', '500+']\n",
    "df['length_bin'] = pd.cut(df['review_length'], bins=bins, labels=labels)\n",
    "\n",
    "accuracy_by_len = df.groupby('length_bin', observed=False)['is_correct'].mean().reset_index()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=accuracy_by_len['length_bin'],\n",
    "    y=accuracy_by_len['is_correct'],\n",
    "    marker_color=ASIIMOV_ORANGE,\n",
    "    name='Accuracy'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Prediction Accuracy by Review Length (Characters)',\n",
    "    xaxis_title='Review Length',\n",
    "    yaxis_title='Accuracy',\n",
    "    yaxis_tickformat='.1%'\n",
    ")\n",
    "\n",
    "apply_asiimov_theme(fig)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}