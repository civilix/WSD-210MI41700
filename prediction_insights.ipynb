{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction Analysis\n",
    "\n",
    "This notebook analyzes the results of sentiment prediction on Counter-Strike 2 reviews.\n",
    "We focus on deriving insights from the model's performance and behavior.\n",
    "\n",
    "**Theme:** Asiimov (Orange, Black, White)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "\n",
    "# Define Asiimov Color Palette\n",
    "# Inspired by the CS:GO Asiimov skin: Distinctive Orange, Black, and White.\n",
    "asiimov_colors = {\n",
    "    'orange': '#ff9d00',\n",
    "    'black': '#1a1a1a',\n",
    "    'white': '#ffffff',\n",
    "    'grey': '#5c5c5c',\n",
    "    'light_grey': '#d1d1d1'\n",
    "}\n",
    "\n",
    "# Set default template or color sequence\n",
    "pio.templates[\"asiimov\"] = go.layout.Template(\n",
    "    layout=go.Layout(\n",
    "        colorway=[asiimov_colors['orange'], asiimov_colors['black'], asiimov_colors['grey']],\n",
    "        plot_bgcolor=asiimov_colors['white'],\n",
    "        paper_bgcolor=asiimov_colors['white'],\n",
    "        font={'color': asiimov_colors['black']},\n",
    "        title={'font': {'color': asiimov_colors['black']}},\n",
    "    )\n",
    ")\n",
    "pio.templates.default = \"asiimov\"\n",
    "\n",
    "print(\"Libraries loaded and Asiimov theme defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prediction results\n",
    "df = pd.read_csv('cs2_10k_predictions.csv')\n",
    "\n",
    "# Display first few rows to verify\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Analysis\n",
    "\n",
    "# Create a column for correctness\n",
    "# voted_up is True/False, predicted_label is 1/0.\n",
    "df['actual_label'] = df['voted_up'].astype(int)\n",
    "df['is_correct'] = df['actual_label'] == df['predicted_label']\n",
    "\n",
    "# Calculate review length\n",
    "df['review_length'] = df['clean_review'].astype(str).apply(len)\n",
    "\n",
    "# Map numeric labels to string for better plotting\n",
    "df['prediction_status'] = df['is_correct'].map({True: 'Correct', False: 'Incorrect'})\n",
    "df['sentiment_label'] = df['actual_label'].map({1: 'Positive', 0: 'Negative'})\n",
    "\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 1: Model Prediction Distribution\n",
    "We investigate how the model performs across positive and negative classes. Does it have a bias towards one sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix-style breakdown\n",
    "confusion_data = df.groupby(['sentiment_label', 'predicted_label']).size().reset_index(name='count')\n",
    "confusion_data['predicted_label_str'] = confusion_data['predicted_label'].map({1: 'Predicted Positive', 0: 'Predicted Negative'})\n",
    "\n",
    "# Plotting with Asiimov colors\n",
    "fig = px.bar(\n",
    "    confusion_data,\n",
    "    x='sentiment_label',\n",
    "    y='count',\n",
    "    color='predicted_label_str',\n",
    "    title='Model Prediction Distribution by Actual Sentiment',\n",
    "    color_discrete_map={\n",
    "        'Predicted Positive': asiimov_colors['orange'],\n",
    "        'Predicted Negative': asiimov_colors['black']\n",
    "    },\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Actual Sentiment\",\n",
    "    yaxis_title=\"Count\",\n",
    "    legend_title=\"Prediction\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary:**\n",
    "This chart visualizes the confusion matrix. \n",
    "- If the orange bar is high for 'Positive' and the black bar is high for 'Negative', the model is doing well.\n",
    "- Significant bars of the 'wrong' color indicate the type of error (False Positive vs False Negative) that is more prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 2: Confidence Distribution\n",
    "Is the model confident when it's wrong? We analyze the distribution of predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of probabilities\n",
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='predicted_prob',\n",
    "    color='prediction_status',\n",
    "    nbins=50,\n",
    "    title='Distribution of Prediction Probabilities (Confidence)',\n",
    "    color_discrete_map={\n",
    "        'Correct': asiimov_colors['orange'],\n",
    "        'Incorrect': asiimov_colors['black']\n",
    "    },\n",
    "    opacity=0.7,\n",
    "    barmode='overlay'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Predicted Probability (0=Negative, 1=Positive)\",\n",
    "    yaxis_title=\"Count\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary:**\n",
    "- **Correct Predictions (Orange):** Should ideally cluster near 0 and 1 (high confidence).\n",
    "- **Incorrect Predictions (Black):** \n",
    "    - If they cluster around 0.5, the model was uncertain.\n",
    "    - If they cluster near 0 or 1, the model was \"confidently wrong\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 3: Playtime vs. Prediction Accuracy\n",
    "Do players with more experience write reviews that are easier or harder to classify? Veterans might use more slang or sarcasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert playtime (minutes) to hours\n",
    "df['playtime_hours'] = df['author_playtime_at_review'] / 60\n",
    "\n",
    "# Create bins for playtime\n",
    "bins = [0, 10, 100, 500, 1000, 5000, 100000]\n",
    "labels = ['0-10h', '10-100h', '100-500h', '500-1k h', '1k-5k h', '5k+ h']\n",
    "df['playtime_category'] = pd.cut(df['playtime_hours'], bins=bins, labels=labels)\n",
    "\n",
    "# Calculate accuracy per bin\n",
    "accuracy_by_playtime = df.groupby('playtime_category', observed=True)['is_correct'].mean().reset_index()\n",
    "\n",
    "fig = px.bar(\n",
    "    accuracy_by_playtime,\n",
    "    x='playtime_category',\n",
    "    y='is_correct',\n",
    "    title='Model Accuracy by Player Experience (Playtime)',\n",
    "    color_discrete_sequence=[asiimov_colors['orange']]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Playtime at Review\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    yaxis_tickformat='.1%'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 4: Review Length vs. Model Confidence\n",
    "Does the model feel more confident when there is more text to analyze?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'confidence' as absolute distance from 0.5 (neutral).\n",
    "df['model_confidence'] = (df['predicted_prob'] - 0.5).abs() * 2  # Scale 0 to 1\n",
    "\n",
    "# Scatter plot of length vs confidence\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='review_length',\n",
    "    y='model_confidence',\n",
    "    color='prediction_status',\n",
    "    title='Review Length vs. Model Confidence',\n",
    "    color_discrete_map={\n",
    "        'Correct': asiimov_colors['orange'],\n",
    "        'Incorrect': asiimov_colors['black']\n",
    "    },\n",
    "    opacity=0.6,\n",
    "    log_x=True # Log scale for length\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Review Length (characters) - Log Scale\",\n",
    "    yaxis_title=\"Model Confidence (0=Unsure, 1=Sure)\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 5: The Impact of \"Funny\" Reviews\n",
    "Are reviews voted as \"Funny\" harder to predict? These reviews often contain sarcasm, ASCII art, or jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning votes_funny\n",
    "df['is_funny'] = df['votes_funny'] > 0\n",
    "accuracy_funny = df.groupby('is_funny')['is_correct'].mean().reset_index()\n",
    "accuracy_funny['is_funny_str'] = accuracy_funny['is_funny'].map({True: 'Rated Funny', False: 'Not Funny'})\n",
    "\n",
    "fig = px.bar(\n",
    "    accuracy_funny,\n",
    "    x='is_funny_str',\n",
    "    y='is_correct',\n",
    "    title='Model Accuracy: Funny vs Normal Reviews',\n",
    "    color='is_funny_str',\n",
    "    color_discrete_map={\n",
    "        'Rated Funny': asiimov_colors['orange'],\n",
    "        'Not Funny': asiimov_colors['black']\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Review Type\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    yaxis_tickformat='.1%',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
